{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-25 13:08:35--  http://thenexio.com/downloads/polar.vocab\n",
      "Resolving thenexio.com (thenexio.com)... 162.215.249.16\n",
      "Connecting to thenexio.com (thenexio.com)|162.215.249.16|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 307463 (300K)\n",
      "Saving to: 'polar.vocab'\n",
      "\n",
      "polar.vocab         100%[===================>] 300.26K  1.01MB/s    in 0.3s    \n",
      "\n",
      "2018-04-25 13:08:36 (1.01 MB/s) - 'polar.vocab' saved [307463/307463]\n",
      "\n",
      "--2018-04-25 13:08:36--  http://thenexio.com/downloads/train.data\n",
      "Resolving thenexio.com (thenexio.com)... 162.215.249.16\n",
      "Connecting to thenexio.com (thenexio.com)|162.215.249.16|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 440600133 (420M)\n",
      "Saving to: 'train.data'\n",
      "\n",
      "train.data          100%[===================>] 420.19M  19.2MB/s    in 22s     \n",
      "\n",
      "2018-04-25 13:08:59 (18.8 MB/s) - 'train.data' saved [440600133/440600133]\n",
      "\n",
      "--2018-04-25 13:08:59--  http://thenexio.com/downloads/test.data\n",
      "Resolving thenexio.com (thenexio.com)... 162.215.249.16\n",
      "Connecting to thenexio.com (thenexio.com)|162.215.249.16|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 392000133 (374M)\n",
      "Saving to: 'test.data'\n",
      "\n",
      "test.data           100%[===================>] 373.84M  19.0MB/s    in 20s     \n",
      "\n",
      "2018-04-25 13:09:20 (18.4 MB/s) - 'test.data' saved [392000133/392000133]\n",
      "\n",
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/87/76e691bbf1759ad6af5831649aae6a8d2fa184a1bcc71018ca6300399e5f/nltk-3.2.5.tar.gz (1.2MB)\n",
      "\u001b[K    100% |################################| 1.2MB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/da/8c/38/a8a36581975f8d03275c49960019f955e4d19fd14ae7e42d3d\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow-gpu==1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/80/094c2f7b05acb1089333d93c41323e93f3296eaa7a785d9848973e4e36bd/tensorflow_gpu-1.5.0-cp27-cp27mu-manylinux1_x86_64.whl (201.9MB)\n",
      "\u001b[K    100% |################################| 201.9MB 7.6kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow-gpu==1.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)\n",
      "\u001b[K    100% |################################| 92kB 14.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: numpy>=1.12.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow-gpu==1.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/ba/d664f7c27c710063b1cdfa0309db8fba98952e3a1ba1991ed98efffe69ed/tensorflow_tensorboard-1.5.1-py2-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |################################| 3.0MB 519kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: html5lib==0.9999999 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-gpu==1.5)\n",
      "Requirement already up-to-date: bleach==1.5.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-gpu==1.5)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-gpu==1.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/ba/f95e3ec83f93919b1437028e989cf3fa5ff4f5cae4a1f62255f71deddb5b/pbr-4.0.2-py2.py3-none-any.whl (98kB)\n",
      "\u001b[K    100% |################################| 102kB 13.0MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.5)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/35/1d/48c0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c\n",
      "Successfully built absl-py\n",
      "Installing collected packages: absl-py, tensorflow-tensorboard, tensorflow-gpu, pbr\n",
      "  Found existing installation: absl-py 0.1.13\n",
      "    Uninstalling absl-py-0.1.13:\n",
      "      Successfully uninstalled absl-py-0.1.13\n",
      "  Found existing installation: tensorflow-gpu 1.7.0\n",
      "    Uninstalling tensorflow-gpu-1.7.0:\n",
      "      Successfully uninstalled tensorflow-gpu-1.7.0\n",
      "  Found existing installation: pbr 4.0.1\n",
      "    Uninstalling pbr-4.0.1:\n",
      "      Successfully uninstalled pbr-4.0.1\n",
      "Successfully installed absl-py-0.2.0 pbr-4.0.2 tensorflow-gpu-1.5.0 tensorflow-tensorboard-1.5.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wget thenexio.com/downloads/polar.vocab\n",
    "!wget thenexio.com/downloads/train.data\n",
    "!wget thenexio.com/downloads/test.data\n",
    "\n",
    "!pip install nltk\n",
    "!pip install tensorflow-gpu==1.5 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def clean(text):\n",
    "\t# split into words without punctuation\n",
    "\t# there's becomes \"there\" \"s\"\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\t# convert to lower case\n",
    "\ttokens = [w.lower() for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\ttokens = [word for word in tokens if not word in stop_words]\n",
    "\t# filter out words less than 1 character\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def cleanv2(text):\n",
    "\t# convert to lower case\n",
    "\ttext = text.lower()\n",
    "\t# change don't to do not, doesn't to does not\n",
    "\ttext = neg_pattern.sub(lambda x: negations_dic[x.group()], text)\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttext = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\t# tokenize\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\t# filter out stop words\n",
    "\ttokens = [word for word in tokens if not word in stop_words]\n",
    "\t# filter out words less than 1 character\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\t\n",
    "\n",
    "# Function to create or fetch vocabulary\n",
    "def make_vocab(vocabFile,directory='./sample'): #floyd\n",
    "\tvocabExists = os.path.isfile(vocabFile)\n",
    "\tif vocabExists:\n",
    "\t\t# Read and return vocab\n",
    "\t\tprint(\"Found vocab file {}\").format(vocabFile)\n",
    "\t\tvocab = load_doc(vocabFile)\n",
    "\t\tvocab = vocab.split('\\n')\n",
    "\t\tprint(\"Vocabulary has {} words\").format(len(vocab))\n",
    "\telse:\n",
    "\t\tprint(\"Did not find vocab file {}\").format(vocabFile)\n",
    "\t\tvocab = Counter()\n",
    "\t\t# walk through all files in the folder\n",
    "\t\tfor path, subdirs, files in os.walk(directory):\n",
    "\t\t\tfor filename in files:\n",
    "\t\t\t\t# create the full path of the file to open\n",
    "\t\t\t\tfilepath =  os.path.join(path, filename)\n",
    "\t\t\t\t# load and clean the doc\n",
    "\t\t\t\tdoc = load_doc(filepath)\n",
    "\t\t\t\ttokens = cleanv2(doc)\n",
    "\t\t\t\tvocab.update(tokens)\n",
    "\n",
    "\t\tprint(\"Number of tokens before filtering freqeuncy of occurance: {}\").format(len(vocab))\n",
    "\t\tvocab = [word for word,freq in vocab.most_common() if freq>2]\n",
    "\t\tprint(\"Number of tokens occuring more than 2 times: {}\").format(len(vocab))\n",
    "\t\t\n",
    "\t\t# Save the vocabulary file\n",
    "\t\t# convert lines to a single blob of text\n",
    "\t\tdata = '\\n'.join(vocab)\n",
    "\t\t# open file\n",
    "\t\tfile = open(vocabFile, 'w+')\n",
    "\t\t# write text\n",
    "\t\tprint(\"Saving vocabulary to {}\").format(vocabFile)\n",
    "\t\tfile.write(data)\n",
    "\t\t# close file\n",
    "\t\tfile.close()\n",
    "\treturn vocab\n",
    "\n",
    "\n",
    "# change all files to BoW representation \n",
    "# based on frequency of words in each review\n",
    "# load all docs in a directory into memory\n",
    "def process_reviews(directory,vocab):\n",
    "\treviews = list()\n",
    "\tsentiment = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor path, subdirs, files in os.walk(directory):\n",
    "\t\t\tfor filename in files:\n",
    "\t\t\t\t# create the full path of the file to open\n",
    "\t\t\t\tfilepath =  os.path.join(path, filename)\n",
    "\t\t\t\t# load the doc\n",
    "\t\t\t\tdoc = load_doc(filepath)\n",
    "\t\t\t\t# clean doc\n",
    "\t\t\t\ttokens = cleanv2(doc)\n",
    "\t\t\t\t# filter by vocab\n",
    "\t\t\t\ttokens = [word for word in tokens if word in vocab]\n",
    "\t\t\t\treview = ' '.join(tokens)\n",
    "\t\t\t\t# append review to reviews\n",
    "\t\t\t\treviews.append(review)\n",
    "\t\t\t\t# Get the sentiment as well\n",
    "\t\t\t\tsentiment.append(1 if 'pos' in filepath else 0)\n",
    "\n",
    "\treturn reviews,sentiment\n",
    "\n",
    "\n",
    "def get_data(data_file,isTrain=True):\n",
    "\tdataset_type = 'train' if isTrain else 'test'\n",
    "\tif os.path.isfile(data_file):\n",
    "\t\tprint(\"Found \"+dataset_type+\" File {}.\").format(data_file)\n",
    "\t\tdata = pickle.load(open(data_file, 'rb'))\n",
    "\t\tX,y = zip(*data)\n",
    "\n",
    "\n",
    "\telse:\n",
    "\t\tprint(\"Did not find \"+dataset_type+\" file.\")\n",
    "\t\tvocab = make_vocab('./polar.vocab','./dataset/train')#floyd\n",
    "\t\tprint(\"Saved Vocabulary\")\n",
    "\t\tprint(\"processing reviews...\")\n",
    "\t\tX,y = process_reviews('./dataset/'+dataset_type,vocab) #floyd\n",
    "\t\tdata = zip(np.array(X),np.array(y))\n",
    "\t\tnp.array(data).dump('./output/'+dataset_type+'.data') #floyd\n",
    "\n",
    "\tX = np.array(X)\n",
    "\ty = np.array(y)\n",
    "\tprint(\"Found {} samples for \"+dataset_type).format(X.shape[0])\n",
    "\n",
    "\treturn X,y\n",
    "\n",
    "def tokenize(X_train,X_test):\n",
    "\tfrom keras.preprocessing.text import Tokenizer\n",
    "\t\n",
    "\tkeras_tokenizer =  Tokenizer()\n",
    "\tkeras_tokenizer.fit_on_texts(X_train)\n",
    "\tX_train = keras_tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\tX_test = keras_tokenizer.texts_to_matrix(X_test, mode='count')\n",
    "\treturn X_train,X_test\n",
    "\n",
    "\n",
    "def get_model(input_shape):\n",
    "\n",
    "\tfrom keras.models import Model\n",
    "\tfrom keras.layers import Input, Dense, Dropout\n",
    "\n",
    "\tinput_layer = Input(shape=(input_shape,))\n",
    "\tx = Dense(50,activation='relu')(input_layer)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\t# x = Dense(128,activation='relu')(x)\n",
    "\t# x = Dropout(0.5)(x)\n",
    "\toutput_layer = Dense(1,activation='sigmoid')(x)\n",
    "\tmodel = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\tmodel.summary()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def train_model(model, X,y,epochs=10):\n",
    "\tfrom keras.callbacks import ModelCheckpoint\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\tsave_model = ModelCheckpoint('count_vectorizer_weights.hdf5', monitor='val_loss',save_best_only=True) #floyd\n",
    "\thist = model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=2, callbacks=[save_model],validation_data=(X_val,y_val),shuffle=True )\n",
    "\treturn hist\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "\tprint(\"Testing model on {}\").format(X_test.shape[0])\n",
    "\tmodel.load_weights('count_vectorizer_weights.hdf5') #floyd\n",
    "\tloss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "def plot_loss(hist):\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tloss = hist.history['loss'] #np.loadtxt('my_cnn_model_loss.csv')\n",
    "\tval_loss = hist.history['val_loss'] #np.loadtxt('my_cnn_model_val_loss.csv')\n",
    "\n",
    "\tplt.plot(loss, linewidth=3, label='train')\n",
    "\tplt.plot(val_loss, linewidth=3, label='valid')\n",
    "\tplt.grid()\n",
    "\tplt.legend()\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.ylabel('loss')\n",
    "\t#plt.ylim(1e-3, 1e-2)\n",
    "\tplt.yscale('log')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found train File ./train.data.\n",
      "Found 25000 samples for train\n",
      "Found test File ./test.data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 samples for test\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 37234)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1861750   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,861,801\n",
      "Trainable params: 1,861,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.3480 - acc: 0.8600 - val_loss: 0.2973 - val_acc: 0.8850\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.1654 - acc: 0.9401 - val_loss: 0.3130 - val_acc: 0.8848\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.0905 - acc: 0.9693 - val_loss: 0.3581 - val_acc: 0.8842\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0579 - acc: 0.9820 - val_loss: 0.4005 - val_acc: 0.8798\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0382 - acc: 0.9892 - val_loss: 0.4402 - val_acc: 0.8808\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0245 - acc: 0.9938 - val_loss: 0.4871 - val_acc: 0.8826\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0227 - acc: 0.9941 - val_loss: 0.5110 - val_acc: 0.8810\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0183 - acc: 0.9949 - val_loss: 0.5264 - val_acc: 0.8800\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.0133 - acc: 0.9968 - val_loss: 0.5915 - val_acc: 0.8820\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.0111 - acc: 0.9973 - val_loss: 0.5984 - val_acc: 0.8792\n",
      "Testing model on 25000\n",
      "Test Accuracy: 87.532000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_file = './train.data' #floyd\n",
    "test_file = './test.data' #floyd\n",
    "\n",
    "X,y = get_data(train_file,True)\n",
    "X_test,y_test = get_data(test_file,False)\n",
    "\n",
    "X,X_test = tokenize(X,X_test)\n",
    "\n",
    "model = get_model(input_shape=X.shape[1])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = train_model(model,X,y,epochs=10)\n",
    "test_model(model,X_test,y_test)\n",
    "plot_loss(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
